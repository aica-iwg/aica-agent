{\rtf1\ansi\ansicpg1252\cocoartf2761
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 This is the code for running Federated Learning with  Differential Privacy\
\
server.py\
This is the server code which resides in the server that starts FL and combines the outputs of client models in order to improve their performance overtime. For this version, the evaluation is done on the client side. The parameters from the clients are \'93hidden\'94 by using differential privacy technique\
\
client.py\
In this implementation, the assumption is that the data and model resides within an independent machine and only shares model parameters with the server. Many clients can be run in parallel but all the models are of the same kind and also the features to be trained should be identical but could of course be of different sizes.\
\
primed_server.py\
This is server code that resides in a server and kick-starts the FL process by incorporating the parameters and weights of a previously trained model so that it does not have to start from scratch. The primed model is retrieved from a model that is typically saved in a file.  The primed_server.py then runs similarly to the  server.py code.\
\
RUNNING THE MODEL\
- the server code is run first typically by - python server.py\
- each of the client codes is run on its own shell by calling - python client.py\
- the final performance metrics (accuracy) will be displayed on the server shell after the last round of FL optimization.\
}